{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLUX LoRA Training in Google Colab\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ Google Colabì—ì„œ FLUX LoRA í›ˆë ¨ì„ ìœ„í•œ í™˜ê²½ì„¤ì • ë° ì‹¤í–‰ ê°€ì´ë“œì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU ì •ë³´ í™•ì¸\n",
    "!nvidia-smi\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (CUDA í˜¸í™˜ì„± í™•ë³´)\n# PyTorchì™€ torchvision CUDA ë²„ì „ í†µì¼\n\n# ê¸°ì¡´ ì„¤ì¹˜ëœ íŒ¨í‚¤ì§€ ì œê±° (ì¶©ëŒ ë°©ì§€)\n!pip uninstall torch torchvision torchaudio -y\n\n# í˜¸í™˜ë˜ëŠ” PyTorch ë²„ì „ ì„¤ì¹˜ (CUDA 11.8)\n!pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118\n\n# ë‹¤ë¥¸ í•„ìˆ˜ íŒ¨í‚¤ì§€ë“¤\n!pip install diffusers transformers peft accelerate datasets\n!pip install Pillow pandas numpy tqdm wandb safetensors\n!pip install sentencepiece protobuf python-dotenv\n\n# ì„¤ì¹˜ í™•ì¸\nimport torch\nprint(f\"âœ… PyTorch version: {torch.__version__}\")\nprint(f\"âœ… CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"âœ… CUDA version: {torch.version.cuda}\")\n    print(f\"âœ… GPU: {torch.cuda.get_device_name()}\")\n    print(f\"âœ… VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# í™˜ê²½ë³€ìˆ˜ ë° GitHub í† í° ì„¤ì •\nimport os\nfrom google.colab import userdata\n\n# ë°©ë²• 1: Colab Secrets ì‚¬ìš© (ê¶Œì¥)\n# ğŸ”‘ í‚¤ ì•„ì´ì½˜ í´ë¦­ â†’ Add new secret:\n# HUGGINGFACE_TOKEN: hf_your_token_here\n# GITHUB_TOKEN: ghp_your_token_here (Private ì €ì¥ì†Œìš©)\n\ntry:\n    HF_TOKEN = userdata.get('HUGGINGFACE_TOKEN')\n    GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n    \n    os.environ['HUGGINGFACE_TOKEN'] = HF_TOKEN\n    os.environ['GITHUB_TOKEN'] = GITHUB_TOKEN\n    \n    print(\"âœ… Tokens loaded from Colab secrets\")\nexcept:\n    # ë°©ë²• 2: ì§ì ‘ ì…ë ¥ (ì„ì‹œìš©)\n    HF_TOKEN = \"hf_your_token_here\"\n    GITHUB_TOKEN = \"ghp_your_token_here\"\n    \n    os.environ['HUGGINGFACE_TOKEN'] = HF_TOKEN\n    os.environ['GITHUB_TOKEN'] = GITHUB_TOKEN\n    \n    print(\"âš ï¸ Tokens set manually\")\n\n# ê¸°íƒ€ í™˜ê²½ë³€ìˆ˜ ì„¤ì •\nos.environ['FLUX_MODEL_NAME'] = 'black-forest-labs/FLUX.1-schnell'\nos.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\nprint(\"âœ… Environment variables configured\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Private GitHub ì €ì¥ì†Œ í´ë¡œë‹\nimport os\n\n# GitHub í† í° í™•ì¸\ngithub_token = os.environ.get('GITHUB_TOKEN')\n\nif github_token and github_token != \"ghp_your_token_here\":\n    # í† í°ì„ ì‚¬ìš©í•œ Private ì €ì¥ì†Œ í´ë¡œë‹\n    !git clone https://{github_token}@github.com/comking2/flux-lora-training.git\n    print(\"âœ… Private repository cloned successfully\")\nelse:\n    # í† í°ì´ ì—†ìœ¼ë©´ Public ì €ì¥ì†Œ ì‹œë„ ë˜ëŠ” ìˆ˜ë™ ì—…ë¡œë“œ ì•ˆë‚´\n    try:\n        !git clone https://github.com/comking2/flux-lora-training.git\n        print(\"âœ… Public repository cloned\")\n    except:\n        print(\"âŒ Repository access failed\")\n        print(\"ğŸ’¡ í•´ê²°ë°©ë²•:\")\n        print(\"1. GitHub Tokenì„ Colab Secretsì— ì¶”ê°€\")\n        print(\"2. ë˜ëŠ” ì €ì¥ì†Œë¥¼ Publicìœ¼ë¡œ ë³€ê²½\")\n        print(\"3. ë˜ëŠ” íŒŒì¼ì„ ì§ì ‘ ì—…ë¡œë“œ\")\n\n# í´ë¡œë‹ëœ ë””ë ‰í† ë¦¬ë¡œ ì´ë™\ntry:\n    %cd flux-lora-training\n    print(\"ğŸ“ Moved to project directory\")\n    \n    # íŒŒì¼ ëª©ë¡ í™•ì¸\n    !ls -la\nexcept:\n    print(\"âš ï¸ Directory not found - please check cloning status\")"
  },
  {
   "cell_type": "code",
   "source": "# ëŒ€ì•ˆ: ê°œë³„ íŒŒì¼ ë‹¤ìš´ë¡œë“œ (í´ë¡œë‹ ì‹¤íŒ¨ ì‹œ)\n# GitHub Tokenì´ ì—†ê±°ë‚˜ Private ì €ì¥ì†Œ ì ‘ê·¼ì´ ì•ˆ ë  ë•Œ ì‚¬ìš©\n\nprint(\"ğŸ”„ Alternative: Downloading individual files...\")\n\n# ì£¼ìš” íŒŒì¼ë“¤ì„ ê°œë³„ì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ\nfiles_to_download = [\n    'train_lora.py',\n    'dataset_processor.py', \n    'inference.py',\n    'config.json',\n    'requirements.txt',\n    '.env.example'\n]\n\nimport requests\nimport os\n\ndef download_file(filename, token=None):\n    if token:\n        url = f\"https://api.github.com/repos/comking2/flux-lora-training/contents/{filename}\"\n        headers = {'Authorization': f'token {token}'}\n        response = requests.get(url, headers=headers)\n        if response.status_code == 200:\n            import base64\n            content = base64.b64decode(response.json()['content']).decode('utf-8')\n            with open(filename, 'w') as f:\n                f.write(content)\n            return True\n    return False\n\n# GitHub APIë¥¼ í†µí•œ ë‹¤ìš´ë¡œë“œ ì‹œë„\ngithub_token = os.environ.get('GITHUB_TOKEN')\ndownloaded = []\n\nfor filename in files_to_download:\n    if download_file(filename, github_token):\n        downloaded.append(filename)\n        print(f\"âœ… Downloaded: {filename}\")\n    else:\n        print(f\"âŒ Failed: {filename}\")\n\nif downloaded:\n    print(f\"\\\\nâœ… Successfully downloaded {len(downloaded)} files\")\n    !ls -la\nelse:\n    print(\"\\\\nğŸ’¡ Please upload files manually or provide GitHub token\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ì—…ë¡œë“œ ë˜ëŠ” ë‹¤ìš´ë¡œë“œ\n",
    "# ë°©ë²• 1: Google Drive ë§ˆìš´íŠ¸\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ë°©ë²• 2: ì§ì ‘ ì—…ë¡œë“œ\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •\n",
    "import torch\n",
    "\n",
    "# GPU ë©”ëª¨ë¦¬ í™•ì¸\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLUX LoRA í›ˆë ¨ ì‹¤í–‰\n",
    "# T4 (16GB) í™˜ê²½ ìµœì í™”\n",
    "\n",
    "!python train_lora.py \\\n",
    "    --data_dir \"/content/drive/MyDrive/training_data\" \\\n",
    "    --output_dir \"./flux_lora_output\" \\\n",
    "    --epochs 10 \\\n",
    "    --batch_size 1 \\\n",
    "    --lora_rank 8 \\\n",
    "    --learning_rate 5e-5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}