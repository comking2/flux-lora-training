{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLUX LoRA Training in Google Colab\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ Google Colabì—ì„œ FLUX LoRA í›ˆë ¨ì„ ìœ„í•œ í™˜ê²½ì„¤ì • ë° ì‹¤í–‰ ê°€ì´ë“œì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU ì •ë³´ í™•ì¸\n",
    "!nvidia-smi\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# ìµœì¢… ì˜ì¡´ì„± í•´ê²° ë°©ë²• (ëª¨ë“  íŒ¨í‚¤ì§€ í˜¸í™˜)\nprint(\"ğŸ¯ Final dependency resolution...\")\n\n# 1. ì™„ì „íˆ ê¹”ë”í•œ ì‹œì‘ì„ ìœ„í•œ ì£¼ìš” íŒ¨í‚¤ì§€ ì œê±°\n!pip uninstall torch torchvision torchaudio numpy opencv-python opencv-python-headless -y\n\n# 2. NumPyë¥¼ ëª¨ë“  íŒ¨í‚¤ì§€ê°€ í˜¸í™˜ë˜ëŠ” ë²„ì „ìœ¼ë¡œ ê³ ì •\n# TensorFlow: <2.1.0, Numba: <2.1, OpenCV: >=2.0\n!pip install \"numpy>=2.0,<2.1\" --force-reinstall\n\n# 3. OpenCV ì„¤ì¹˜ (NumPy 2.0 í˜¸í™˜)\n!pip install opencv-python-headless>=4.9.0.80\n\n# 4. PyTorch ì„¤ì¹˜ (ëª¨ë“  ML íŒ¨í‚¤ì§€ ì˜ì¡´ì„± ë§Œì¡±)\n!pip install torch>=2.0.0 torchvision>=0.15.0 torchaudio --index-url https://download.pytorch.org/whl/cu121\n\n# 5. ML íŒ¨í‚¤ì§€ë“¤ ìˆœì°¨ ì„¤ì¹˜\n!pip install transformers>=4.21.0\n!pip install diffusers>=0.21.0\n!pip install peft>=0.4.0\n!pip install accelerate>=0.21.0\n\n# 6. ê¸°íƒ€ í•„ìˆ˜ íŒ¨í‚¤ì§€\n!pip install datasets Pillow pandas tqdm wandb safetensors\n!pip install sentencepiece protobuf python-dotenv\n\n# 7. ì˜ì¡´ì„± ì²´í¬\nprint(\"ğŸ” Checking dependencies...\")\n!pip check\n\nprint(\"âœ… Installation completed with compatible versions\")\nprint(\"âš ï¸ Please restart runtime before proceeding\")"
  },
  {
   "cell_type": "code",
   "source": "# ğŸš€ ê°€ì¥ ê°„ë‹¨í•œ í•´ê²°ì±… (ì¶”ì²œ)\nprint(\"ğŸ’¡ Simplest solution: Use default Colab packages\")\n\n# Colab ê¸°ë³¸ í™˜ê²½ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê³  í•„ìˆ˜ íŒ¨í‚¤ì§€ë§Œ ì¶”ê°€\n# NumPyë‚˜ OpenCV ê±´ë“œë¦¬ì§€ ì•ŠìŒ\n\n# PyTorchë§Œ í•„ìš”ì‹œ ì—…ë°ì´íŠ¸\n!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 --upgrade\n\n# FLUX ê´€ë ¨ í•„ìˆ˜ íŒ¨í‚¤ì§€ë§Œ ì„¤ì¹˜\n!pip install diffusers --upgrade\n!pip install transformers --upgrade  \n!pip install peft accelerate\n!pip install sentencepiece protobuf python-dotenv\n\n# ì˜ì¡´ì„± ê²½ê³  ë¬´ì‹œí•˜ê³  ì§„í–‰\nprint(\"âœ… Essential packages installed\")\nprint(\"âš ï¸ Dependency warnings can be safely ignored\")\nprint(\"ğŸ’¡ FLUX training should work with this setup\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ëŒ€ì•ˆ: ê¹”ë”í•œ í™˜ê²½ìœ¼ë¡œ ì‹œì‘ (ê¶Œì¥)\nprint(\"ğŸ”„ Alternative: Clean installation...\")\n\n# Colab ëŸ°íƒ€ì„ì„ ì™„ì „íˆ ë¦¬ì…‹í•˜ëŠ” ê²ƒì´ ê°€ì¥ í™•ì‹¤í•¨\nprint(\"ğŸ’¡ Most reliable method:\")\nprint(\"1. Runtime â†’ Disconnect and delete runtime\")  \nprint(\"2. Runtime â†’ Connect (new clean environment)\")\nprint(\"3. Run this installation cell\")\n\n# ìƒˆ í™˜ê²½ì—ì„œ ì˜¬ë°”ë¥¸ ìˆœì„œë¡œ ì„¤ì¹˜\n!pip install --upgrade pip setuptools wheel\n\n# NumPy 2.x ê³„ì—´ë¡œ í†µì¼ (ëª¨ë“  íŒ¨í‚¤ì§€ê°€ ìš”êµ¬í•˜ëŠ” ë²„ì „)\n!pip install \"numpy>=2.0,<2.3\"\n\n# PyTorch (ìµœì‹  ì•ˆì • ë²„ì „)\n!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n\n# í•µì‹¬ ML íŒ¨í‚¤ì§€ë“¤ (ìˆœì„œ ì¤‘ìš”)\n!pip install transformers\n!pip install diffusers \n!pip install peft accelerate\n\n# ê¸°íƒ€ í•„ìˆ˜ íŒ¨í‚¤ì§€ë“¤\n!pip install datasets Pillow pandas tqdm wandb safetensors\n!pip install sentencepiece protobuf python-dotenv\n\nprint(\"âœ… Clean installation completed\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ì„¤ì¹˜ ê²€ì¦ (ëŸ°íƒ€ì„ ì¬ì‹œì‘ í›„ ì‹¤í–‰)\nprint(\"ğŸ” Verifying installation...\")\n\ntry:\n    import numpy as np\n    print(f\"âœ… NumPy version: {np.__version__}\")\n    \n    import torch\n    print(f\"âœ… PyTorch version: {torch.__version__}\")\n    print(f\"âœ… CUDA available: {torch.cuda.is_available()}\")\n    \n    if torch.cuda.is_available():\n        print(f\"âœ… CUDA version: {torch.version.cuda}\")\n        print(f\"âœ… GPU: {torch.cuda.get_device_name()}\")\n        print(f\"âœ… VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n    \n    from diffusers import FluxPipeline\n    print(\"âœ… Diffusers imported successfully\")\n    \n    import transformers\n    print(f\"âœ… Transformers version: {transformers.__version__}\")\n    \n    import peft\n    print(f\"âœ… PEFT version: {peft.__version__}\")\n    \n    print(\"\\\\nğŸ‰ All packages installed correctly!\")\n    print(\"ğŸ’¡ You can now proceed with the training\")\n    \nexcept ImportError as e:\n    print(f\"âŒ Import error: {e}\")\n    print(\"ğŸ’¡ Please check package installation\")\nexcept Exception as e:\n    print(f\"âŒ Error: {e}\")\n    print(\"ğŸ’¡ Please restart runtime and try again\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# í™˜ê²½ë³€ìˆ˜ ë° GitHub í† í° ì„¤ì •\nimport os\nfrom google.colab import userdata\n\n# ë°©ë²• 1: Colab Secrets ì‚¬ìš© (ê¶Œì¥)\n# ğŸ”‘ í‚¤ ì•„ì´ì½˜ í´ë¦­ â†’ Add new secret:\n# HUGGINGFACE_TOKEN: hf_your_token_here\n# GITHUB_TOKEN: ghp_your_token_here (Private ì €ì¥ì†Œìš©)\n\ntry:\n    HF_TOKEN = userdata.get('HUGGINGFACE_TOKEN')\n    GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n    \n    os.environ['HUGGINGFACE_TOKEN'] = HF_TOKEN\n    os.environ['GITHUB_TOKEN'] = GITHUB_TOKEN\n    \n    print(\"âœ… Tokens loaded from Colab secrets\")\nexcept:\n    # ë°©ë²• 2: ì§ì ‘ ì…ë ¥ (ì„ì‹œìš©)\n    HF_TOKEN = \"hf_your_token_here\"\n    GITHUB_TOKEN = \"ghp_your_token_here\"\n    \n    os.environ['HUGGINGFACE_TOKEN'] = HF_TOKEN\n    os.environ['GITHUB_TOKEN'] = GITHUB_TOKEN\n    \n    print(\"âš ï¸ Tokens set manually\")\n\n# ê¸°íƒ€ í™˜ê²½ë³€ìˆ˜ ì„¤ì •\nos.environ['FLUX_MODEL_NAME'] = 'black-forest-labs/FLUX.1-schnell'\nos.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\nprint(\"âœ… Environment variables configured\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Private GitHub ì €ì¥ì†Œ í´ë¡œë‹\nimport os\n\n# GitHub í† í° í™•ì¸\ngithub_token = os.environ.get('GITHUB_TOKEN')\n\nif github_token and github_token != \"ghp_your_token_here\":\n    # í† í°ì„ ì‚¬ìš©í•œ Private ì €ì¥ì†Œ í´ë¡œë‹\n    !git clone https://{github_token}@github.com/comking2/flux-lora-training.git\n    print(\"âœ… Private repository cloned successfully\")\nelse:\n    # í† í°ì´ ì—†ìœ¼ë©´ Public ì €ì¥ì†Œ ì‹œë„ ë˜ëŠ” ìˆ˜ë™ ì—…ë¡œë“œ ì•ˆë‚´\n    try:\n        !git clone https://github.com/comking2/flux-lora-training.git\n        print(\"âœ… Public repository cloned\")\n    except:\n        print(\"âŒ Repository access failed\")\n        print(\"ğŸ’¡ í•´ê²°ë°©ë²•:\")\n        print(\"1. GitHub Tokenì„ Colab Secretsì— ì¶”ê°€\")\n        print(\"2. ë˜ëŠ” ì €ì¥ì†Œë¥¼ Publicìœ¼ë¡œ ë³€ê²½\")\n        print(\"3. ë˜ëŠ” íŒŒì¼ì„ ì§ì ‘ ì—…ë¡œë“œ\")\n\n# í´ë¡œë‹ëœ ë””ë ‰í† ë¦¬ë¡œ ì´ë™\ntry:\n    %cd flux-lora-training\n    print(\"ğŸ“ Moved to project directory\")\n    \n    # íŒŒì¼ ëª©ë¡ í™•ì¸\n    !ls -la\nexcept:\n    print(\"âš ï¸ Directory not found - please check cloning status\")"
  },
  {
   "cell_type": "code",
   "source": "# ëŒ€ì•ˆ: ê°œë³„ íŒŒì¼ ë‹¤ìš´ë¡œë“œ (í´ë¡œë‹ ì‹¤íŒ¨ ì‹œ)\n# GitHub Tokenì´ ì—†ê±°ë‚˜ Private ì €ì¥ì†Œ ì ‘ê·¼ì´ ì•ˆ ë  ë•Œ ì‚¬ìš©\n\nprint(\"ğŸ”„ Alternative: Downloading individual files...\")\n\n# ì£¼ìš” íŒŒì¼ë“¤ì„ ê°œë³„ì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œ\nfiles_to_download = [\n    'train_lora.py',\n    'dataset_processor.py', \n    'inference.py',\n    'config.json',\n    'requirements.txt',\n    '.env.example'\n]\n\nimport requests\nimport os\n\ndef download_file(filename, token=None):\n    if token:\n        url = f\"https://api.github.com/repos/comking2/flux-lora-training/contents/{filename}\"\n        headers = {'Authorization': f'token {token}'}\n        response = requests.get(url, headers=headers)\n        if response.status_code == 200:\n            import base64\n            content = base64.b64decode(response.json()['content']).decode('utf-8')\n            with open(filename, 'w') as f:\n                f.write(content)\n            return True\n    return False\n\n# GitHub APIë¥¼ í†µí•œ ë‹¤ìš´ë¡œë“œ ì‹œë„\ngithub_token = os.environ.get('GITHUB_TOKEN')\ndownloaded = []\n\nfor filename in files_to_download:\n    if download_file(filename, github_token):\n        downloaded.append(filename)\n        print(f\"âœ… Downloaded: {filename}\")\n    else:\n        print(f\"âŒ Failed: {filename}\")\n\nif downloaded:\n    print(f\"\\\\nâœ… Successfully downloaded {len(downloaded)} files\")\n    !ls -la\nelse:\n    print(\"\\\\nğŸ’¡ Please upload files manually or provide GitHub token\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ì—…ë¡œë“œ ë˜ëŠ” ë‹¤ìš´ë¡œë“œ\n",
    "# ë°©ë²• 1: Google Drive ë§ˆìš´íŠ¸\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ë°©ë²• 2: ì§ì ‘ ì—…ë¡œë“œ\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •\n",
    "import torch\n",
    "\n",
    "# GPU ë©”ëª¨ë¦¬ í™•ì¸\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLUX LoRA í›ˆë ¨ ì‹¤í–‰\n",
    "# T4 (16GB) í™˜ê²½ ìµœì í™”\n",
    "\n",
    "!python train_lora.py \\\n",
    "    --data_dir \"/content/drive/MyDrive/training_data\" \\\n",
    "    --output_dir \"./flux_lora_output\" \\\n",
    "    --epochs 10 \\\n",
    "    --batch_size 1 \\\n",
    "    --lora_rank 8 \\\n",
    "    --learning_rate 5e-5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}