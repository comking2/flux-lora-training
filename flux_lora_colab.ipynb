{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLUX LoRA Training in Google Colab\n",
    "\n",
    "이 노트북은 Google Colab에서 FLUX LoRA 훈련을 위한 환경설정 및 실행 가이드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 정보 확인\n",
    "!nvidia-smi\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 최종 의존성 해결 방법 (모든 패키지 호환)\nprint(\"🎯 Final dependency resolution...\")\n\n# 1. 완전히 깔끔한 시작을 위한 주요 패키지 제거\n!pip uninstall torch torchvision torchaudio numpy opencv-python opencv-python-headless -y\n\n# 2. NumPy를 모든 패키지가 호환되는 버전으로 고정\n# TensorFlow: <2.1.0, Numba: <2.1, OpenCV: >=2.0\n!pip install \"numpy>=2.0,<2.1\" --force-reinstall\n\n# 3. OpenCV 설치 (NumPy 2.0 호환)\n!pip install opencv-python-headless>=4.9.0.80\n\n# 4. PyTorch 설치 (모든 ML 패키지 의존성 만족)\n!pip install torch>=2.0.0 torchvision>=0.15.0 torchaudio --index-url https://download.pytorch.org/whl/cu121\n\n# 5. ML 패키지들 순차 설치\n!pip install transformers>=4.21.0\n!pip install diffusers>=0.21.0\n!pip install peft>=0.4.0\n!pip install accelerate>=0.21.0\n\n# 6. 기타 필수 패키지\n!pip install datasets Pillow pandas tqdm wandb safetensors\n!pip install sentencepiece protobuf python-dotenv\n\n# 7. 의존성 체크\nprint(\"🔍 Checking dependencies...\")\n!pip check\n\nprint(\"✅ Installation completed with compatible versions\")\nprint(\"⚠️ Please restart runtime before proceeding\")"
  },
  {
   "cell_type": "code",
   "source": "# 🚀 가장 간단한 해결책 (추천)\nprint(\"💡 Simplest solution: Use default Colab packages\")\n\n# Colab 기본 환경 그대로 사용하고 필수 패키지만 추가\n# NumPy나 OpenCV 건드리지 않음\n\n# PyTorch만 필요시 업데이트\n!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 --upgrade\n\n# FLUX 관련 필수 패키지만 설치\n!pip install diffusers --upgrade\n!pip install transformers --upgrade  \n!pip install peft accelerate\n!pip install sentencepiece protobuf python-dotenv\n\n# 의존성 경고 무시하고 진행\nprint(\"✅ Essential packages installed\")\nprint(\"⚠️ Dependency warnings can be safely ignored\")\nprint(\"💡 FLUX training should work with this setup\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 대안: 깔끔한 환경으로 시작 (권장)\nprint(\"🔄 Alternative: Clean installation...\")\n\n# Colab 런타임을 완전히 리셋하는 것이 가장 확실함\nprint(\"💡 Most reliable method:\")\nprint(\"1. Runtime → Disconnect and delete runtime\")  \nprint(\"2. Runtime → Connect (new clean environment)\")\nprint(\"3. Run this installation cell\")\n\n# 새 환경에서 올바른 순서로 설치\n!pip install --upgrade pip setuptools wheel\n\n# NumPy 2.x 계열로 통일 (모든 패키지가 요구하는 버전)\n!pip install \"numpy>=2.0,<2.3\"\n\n# PyTorch (최신 안정 버전)\n!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n\n# 핵심 ML 패키지들 (순서 중요)\n!pip install transformers\n!pip install diffusers \n!pip install peft accelerate\n\n# 기타 필수 패키지들\n!pip install datasets Pillow pandas tqdm wandb safetensors\n!pip install sentencepiece protobuf python-dotenv\n\nprint(\"✅ Clean installation completed\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 설치 검증 (런타임 재시작 후 실행)\nprint(\"🔍 Verifying installation...\")\n\ntry:\n    import numpy as np\n    print(f\"✅ NumPy version: {np.__version__}\")\n    \n    import torch\n    print(f\"✅ PyTorch version: {torch.__version__}\")\n    print(f\"✅ CUDA available: {torch.cuda.is_available()}\")\n    \n    if torch.cuda.is_available():\n        print(f\"✅ CUDA version: {torch.version.cuda}\")\n        print(f\"✅ GPU: {torch.cuda.get_device_name()}\")\n        print(f\"✅ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n    \n    from diffusers import FluxPipeline\n    print(\"✅ Diffusers imported successfully\")\n    \n    import transformers\n    print(f\"✅ Transformers version: {transformers.__version__}\")\n    \n    import peft\n    print(f\"✅ PEFT version: {peft.__version__}\")\n    \n    print(\"\\\\n🎉 All packages installed correctly!\")\n    print(\"💡 You can now proceed with the training\")\n    \nexcept ImportError as e:\n    print(f\"❌ Import error: {e}\")\n    print(\"💡 Please check package installation\")\nexcept Exception as e:\n    print(f\"❌ Error: {e}\")\n    print(\"💡 Please restart runtime and try again\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 환경변수 및 GitHub 토큰 설정\nimport os\nfrom google.colab import userdata\n\n# 방법 1: Colab Secrets 사용 (권장)\n# 🔑 키 아이콘 클릭 → Add new secret:\n# HUGGINGFACE_TOKEN: hf_your_token_here\n# GITHUB_TOKEN: ghp_your_token_here (Private 저장소용)\n\ntry:\n    HF_TOKEN = userdata.get('HUGGINGFACE_TOKEN')\n    GITHUB_TOKEN = userdata.get('GITHUB_TOKEN')\n    \n    os.environ['HUGGINGFACE_TOKEN'] = HF_TOKEN\n    os.environ['GITHUB_TOKEN'] = GITHUB_TOKEN\n    \n    print(\"✅ Tokens loaded from Colab secrets\")\nexcept:\n    # 방법 2: 직접 입력 (임시용)\n    HF_TOKEN = \"hf_your_token_here\"\n    GITHUB_TOKEN = \"ghp_your_token_here\"\n    \n    os.environ['HUGGINGFACE_TOKEN'] = HF_TOKEN\n    os.environ['GITHUB_TOKEN'] = GITHUB_TOKEN\n    \n    print(\"⚠️ Tokens set manually\")\n\n# 기타 환경변수 설정\nos.environ['FLUX_MODEL_NAME'] = 'black-forest-labs/FLUX.1-schnell'\nos.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\nprint(\"✅ Environment variables configured\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Private GitHub 저장소 클로닝\nimport os\n\n# GitHub 토큰 확인\ngithub_token = os.environ.get('GITHUB_TOKEN')\n\nif github_token and github_token != \"ghp_your_token_here\":\n    # 토큰을 사용한 Private 저장소 클로닝\n    !git clone https://{github_token}@github.com/comking2/flux-lora-training.git\n    print(\"✅ Private repository cloned successfully\")\nelse:\n    # 토큰이 없으면 Public 저장소 시도 또는 수동 업로드 안내\n    try:\n        !git clone https://github.com/comking2/flux-lora-training.git\n        print(\"✅ Public repository cloned\")\n    except:\n        print(\"❌ Repository access failed\")\n        print(\"💡 해결방법:\")\n        print(\"1. GitHub Token을 Colab Secrets에 추가\")\n        print(\"2. 또는 저장소를 Public으로 변경\")\n        print(\"3. 또는 파일을 직접 업로드\")\n\n# 클로닝된 디렉토리로 이동\ntry:\n    %cd flux-lora-training\n    print(\"📁 Moved to project directory\")\n    \n    # 파일 목록 확인\n    !ls -la\nexcept:\n    print(\"⚠️ Directory not found - please check cloning status\")"
  },
  {
   "cell_type": "code",
   "source": "# 대안: 개별 파일 다운로드 (클로닝 실패 시)\n# GitHub Token이 없거나 Private 저장소 접근이 안 될 때 사용\n\nprint(\"🔄 Alternative: Downloading individual files...\")\n\n# 주요 파일들을 개별적으로 다운로드\nfiles_to_download = [\n    'train_lora.py',\n    'dataset_processor.py', \n    'inference.py',\n    'config.json',\n    'requirements.txt',\n    '.env.example'\n]\n\nimport requests\nimport os\n\ndef download_file(filename, token=None):\n    if token:\n        url = f\"https://api.github.com/repos/comking2/flux-lora-training/contents/{filename}\"\n        headers = {'Authorization': f'token {token}'}\n        response = requests.get(url, headers=headers)\n        if response.status_code == 200:\n            import base64\n            content = base64.b64decode(response.json()['content']).decode('utf-8')\n            with open(filename, 'w') as f:\n                f.write(content)\n            return True\n    return False\n\n# GitHub API를 통한 다운로드 시도\ngithub_token = os.environ.get('GITHUB_TOKEN')\ndownloaded = []\n\nfor filename in files_to_download:\n    if download_file(filename, github_token):\n        downloaded.append(filename)\n        print(f\"✅ Downloaded: {filename}\")\n    else:\n        print(f\"❌ Failed: {filename}\")\n\nif downloaded:\n    print(f\"\\\\n✅ Successfully downloaded {len(downloaded)} files\")\n    !ls -la\nelse:\n    print(\"\\\\n💡 Please upload files manually or provide GitHub token\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 업로드 또는 다운로드\n",
    "# 방법 1: Google Drive 마운트\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 방법 2: 직접 업로드\n",
    "# from google.colab import files\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 메모리 최적화 설정\n",
    "import torch\n",
    "\n",
    "# GPU 메모리 확인\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "    \n",
    "    # 메모리 캐시 정리\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLUX LoRA 훈련 실행\n",
    "# T4 (16GB) 환경 최적화\n",
    "\n",
    "!python train_lora.py \\\n",
    "    --data_dir \"/content/drive/MyDrive/training_data\" \\\n",
    "    --output_dir \"./flux_lora_output\" \\\n",
    "    --epochs 10 \\\n",
    "    --batch_size 1 \\\n",
    "    --lora_rank 8 \\\n",
    "    --learning_rate 5e-5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}