# ğŸ¨ Flux LoRA Training Pipeline

FLUX.1 ëª¨ë¸ì„ ìœ„í•œ LoRA(Low-Rank Adaptation) í›ˆë ¨ íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.

## âš ï¸ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### ìµœì†Œ ìš”êµ¬ì‚¬ì–‘:
- **GPU**: 16GB VRAM ì´ìƒ (RTX 4080, A100 ë“±)
- **RAM**: 32GB ì´ìƒ
- **ì €ì¥ê³µê°„**: 50GB ì—¬ìœ ê³µê°„

### ê¶Œì¥ ì‚¬ì–‘:
- **GPU**: 24GB+ VRAM (RTX 4090, A100 40GB ë“±)
- **RAM**: 64GB ì´ìƒ

### ğŸ”¥ Google Colab ì‚¬ìš© ê¶Œì¥!
- **Colab Pro**: A100 40GB GPU + 51GB RAM
- **Colab Free**: T4 16GB GPU + 12GB RAM (ìµœì í™” í•„ìš”)

## êµ¬ì¡°

```
flux_custom/
â”œâ”€â”€ dataset_processor.py    # ë°ì´í„°ì…‹ ì²˜ë¦¬ ë° ë¡œë”©
â”œâ”€â”€ train_lora.py          # LoRA í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ inference.py           # ì¶”ë¡  ë° ì´ë¯¸ì§€ ìƒì„±
â”œâ”€â”€ config.json           # í›ˆë ¨ ì„¤ì • íŒŒì¼
â”œâ”€â”€ requirements.txt      # í•„ìš”í•œ íŒ¨í‚¤ì§€ ëª©ë¡
â”œâ”€â”€ train.sh             # í›ˆë ¨ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ generate.sh          # ìƒì„± ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ README.md            # ì´ íŒŒì¼
```

## ì„¤ì¹˜

1. ê°€ìƒí™˜ê²½ í™œì„±í™”:
```bash
source env/bin/activate  # Linux/Mac
# ë˜ëŠ”
env\Scripts\activate     # Windows
```

2. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜:
```bash
pip install -r requirements.txt
```

## ë°ì´í„°ì…‹ ì¤€ë¹„

### ë°©ë²• 1: ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒ íŒŒì¼
ê° ì´ë¯¸ì§€ì— ëŒ€í•´ ë™ì¼í•œ ì´ë¦„ì˜ .txt íŒŒì¼ì„ ìƒì„±:
```
training_data/
â”œâ”€â”€ image1.jpg
â”œâ”€â”€ image1.txt
â”œâ”€â”€ image2.png
â”œâ”€â”€ image2.txt
â””â”€â”€ ...
```

### ë°©ë²• 2: JSON í˜•ì‹
```json
[
  {
    "image": "image1.jpg",
    "caption": "ì´ë¯¸ì§€ ì„¤ëª…"
  },
  {
    "image": "image2.png",
    "caption": "ë‹¤ë¥¸ ì´ë¯¸ì§€ ì„¤ëª…"
  }
]
```

### ë°©ë²• 3: CSV í˜•ì‹
```csv
image,caption
image1.jpg,"ì´ë¯¸ì§€ ì„¤ëª…"
image2.png,"ë‹¤ë¥¸ ì´ë¯¸ì§€ ì„¤ëª…"
```

## í›ˆë ¨

### ê¸°ë³¸ í›ˆë ¨:
```bash
./train.sh --data_dir ./training_data
```

### ê³ ê¸‰ ì˜µì…˜:
```bash
./train.sh --data_dir ./training_data \
          --epochs 20 \
          --batch_size 2 \
          --learning_rate 5e-5 \
          --lora_rank 32 \
          --use_wandb
```

### Python ì§ì ‘ ì‹¤í–‰:
```bash
python train_lora.py --data_dir ./training_data \
                     --output_dir ./my_lora_model \
                     --epochs 15 \
                     --batch_size 1 \
                     --learning_rate 1e-4
```

## ì¶”ë¡ 

### ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ ìƒì„±:
```bash
./generate.sh --lora_path ./flux_lora_output/flux-lora-final \
              --prompt "a beautiful landscape with mountains"
```

### ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ ë°°ì¹˜ ìƒì„±:
```bash
# prompts.txt íŒŒì¼ì— í”„ë¡¬í”„íŠ¸ë“¤ì„ í•œ ì¤„ì”© ì‘ì„±
./generate.sh --lora_path ./flux_lora_output/flux-lora-final \
              --prompts_file prompts.txt
```

### í…ŒìŠ¤íŠ¸ ì‹¤í–‰:
```bash
./generate.sh --lora_path ./flux_lora_output/flux-lora-final --test
```

### ëª¨ë¸ ë¹„êµ:
```bash
./generate.sh --lora_path ./flux_lora_output/flux-lora-final --compare
```

### Python ì§ì ‘ ì‹¤í–‰:
```bash
python inference.py --lora_path ./flux_lora_output/flux-lora-final \
                    --prompt "your prompt here" \
                    --height 1024 \
                    --width 1024 \
                    --steps 50
```

## ì„¤ì •

`config.json` íŒŒì¼ì„ ìˆ˜ì •í•˜ì—¬ ê¸°ë³¸ ì„¤ì •ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```json
{
  "model_name": "black-forest-labs/FLUX.1-dev",
  "batch_size": 1,
  "epochs": 10,
  "learning_rate": 1e-4,
  "lora_rank": 16,
  "lora_alpha": 32,
  "image_size": 512
}
```

## ì£¼ìš” ë§¤ê°œë³€ìˆ˜

### í›ˆë ¨ ë§¤ê°œë³€ìˆ˜:
- `--epochs`: í›ˆë ¨ ì—í¬í¬ ìˆ˜
- `--batch_size`: ë°°ì¹˜ í¬ê¸° (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •)
- `--learning_rate`: í•™ìŠµë¥ 
- `--lora_rank`: LoRA ë­í¬ (ì‘ì„ìˆ˜ë¡ íŒŒë¼ë¯¸í„° ì ìŒ)
- `--lora_alpha`: LoRA ì•ŒíŒŒ (ì¼ë°˜ì ìœ¼ë¡œ rankì˜ 2ë°°)

### ì¶”ë¡  ë§¤ê°œë³€ìˆ˜:
- `--height`, `--width`: ìƒì„± ì´ë¯¸ì§€ í¬ê¸°
- `--steps`: ì¶”ë¡  ìŠ¤í… ìˆ˜ (ë§ì„ìˆ˜ë¡ í’ˆì§ˆ ì¢‹ì§€ë§Œ ëŠë¦¼)
- `--guidance_scale`: ê°€ì´ë˜ìŠ¤ ìŠ¤ì¼€ì¼ (7.5ê°€ ì¼ë°˜ì )
- `--seed`: ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ

## ë©”ëª¨ë¦¬ ìš”êµ¬ì‚¬í•­

- **ìµœì†Œ**: 12GB VRAM (batch_size=1, fp16)
- **ê¶Œì¥**: 24GB VRAM (batch_size=2-4)
- **CPU**: 16GB RAM ì´ìƒ

## ë¬¸ì œí•´ê²°

1. **CUDA ë©”ëª¨ë¦¬ ë¶€ì¡±**: batch_sizeë¥¼ ì¤„ì´ê±°ë‚˜ image_sizeë¥¼ 512ë¡œ ì„¤ì •
2. **í›ˆë ¨ ì†ë„ ëŠë¦¼**: gradient_accumulation_stepsë¥¼ ì‚¬ìš©í•˜ì—¬ effective batch size ì¦ê°€
3. **í’ˆì§ˆ ë¬¸ì œ**: epochs ìˆ˜ ì¦ê°€, learning_rate ì¡°ì •, ë” ë§ì€ í›ˆë ¨ ë°ì´í„° ì‚¬ìš©

## ì¶œë ¥ êµ¬ì¡°

í›ˆë ¨ í›„ ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¡œ ì¶œë ¥ë©ë‹ˆë‹¤:

```
flux_lora_output/
â”œâ”€â”€ flux-lora-final/
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”œâ”€â”€ adapter_model.safetensors
â”‚   â””â”€â”€ training_config.json
â”œâ”€â”€ checkpoint-500/
â”œâ”€â”€ checkpoint-1000/
â””â”€â”€ ...
```